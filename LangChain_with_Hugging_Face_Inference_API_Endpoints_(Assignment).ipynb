{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martefrain10/Langchain_Open_Source/blob/main/LangChain_with_Hugging_Face_Inference_API_Endpoints_(Assignment).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain with Open Source LLM and Open Source Embeddings & LangSmith\n",
        "\n",
        "In the following notebook we will dive into the world of Open Source models hosted on Hugging Face's [inference endpoints](https://ui.endpoints.huggingface.co/).\n",
        "\n",
        "The notebook will be broken into the following parts:\n",
        "\n",
        "- 🤝 Breakout Room #1:\n",
        "  1. Set-up Hugging Face Infrence Endpoints\n",
        "  2. Install required libraries\n",
        "  3. Set Environment Variables\n",
        "  4. Testing our Hugging Face Inference Endpoint\n",
        "  5. Creating LangChain components powered by the endpoints\n",
        "  6. Retrieving data from Arxiv\n",
        "  7. Creating a simple RAG pipeline with [LangChain v0.1.0](https://blog.langchain.dev/langchain-v0-1-0/)\n",
        "  \n",
        "\n",
        "- 🤝 Breakout Room #2:\n",
        "  1. Set-up LangSmith\n",
        "  2. Creating a LangSmith dataset\n",
        "  3. Creating a custom evaluator\n",
        "  4. Initializing our evaluator config\n",
        "  5. Evaluating our RAG pipeline"
      ],
      "metadata": {
        "id": "ctrwj6Cj24Zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤝 Breakout Room #1"
      ],
      "metadata": {
        "id": "AduTna3oCbP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Set-up Hugging Face Infrence Endpoints\n",
        "\n",
        "Please follow the instructions provided [here](https://github.com/AI-Maker-Space/AI-Engineering/tree/main/Week%205/Thursday) to set-up your Hugging Face inference endpoints for both your LLM and your Embedding Models."
      ],
      "metadata": {
        "id": "ENUY6OSnDy7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Install required libraries\n",
        "\n",
        "Now we've got to get our required libraries!\n",
        "\n",
        "We'll start with our `langchain` and `huggingface` dependencies.\n",
        "\n"
      ],
      "metadata": {
        "id": "-spIWt2J3Quk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-core langchain-community langchain_openai huggingface-hub requests -q -U"
      ],
      "metadata": {
        "id": "EwGLnp31jXJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbe98b7f-70e0-47de-fd1d-35fb08ea0b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/867.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.7/867.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m819.2/867.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.7/116.7 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can grab some miscellaneous dependencies that will help us power our RAG pipeline!"
      ],
      "metadata": {
        "id": "yPXElql-EE9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv pymupdf faiss-cpu -q -U"
      ],
      "metadata": {
        "id": "FMJqq8SYt34V",
        "outputId": "74ba9e30-1883-4da5-825a-66525cbaa3ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Set Environment Variables\n",
        "\n",
        "We'll need to set our `HF_TOKEN` so that we can send requests to our protected API endpoint.\n",
        "\n",
        "We'll also set-up our OpenAI API key, which we'll leverage later.\n",
        "\n"
      ],
      "metadata": {
        "id": "SpZTBLwK3TIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HuggingFace Write Token: \")"
      ],
      "metadata": {
        "id": "NspG8I0XlFTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "626f9ead-21dd-444c-fa62-c9d6ad7ab867"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HuggingFace Write Token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giMejsXN7EKb",
        "outputId": "d1b4fcd4-ba56-4a46-ee55-7599ed6504c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Testing our Hugging Face Inference Endpoint\n",
        "\n",
        "Let's submit a sample request to the Hugging Face Inference endpoint!"
      ],
      "metadata": {
        "id": "-3M7TzXs3WsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_api_gateway = \"https://p3w8fylc7tahgjkf.us-east-1.aws.endpoints.huggingface.cloud\" # << YOUR ENDPOINT URL HERE"
      ],
      "metadata": {
        "id": "uyFgZVUSEexW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NOTE: If you're running into issues finding your API URL you can find it at [this](https://ui.endpoints.huggingface.co/) link.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "![image](https://i.imgur.com/XyZhOv8.png)"
      ],
      "metadata": {
        "id": "EvnMlmEsEiqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "max_new_tokens = 256\n",
        "top_p = 0.9\n",
        "temperature = 0.1\n",
        "\n",
        "prompt = \"Hello! How are you?\"\n",
        "\n",
        "json_body = {\n",
        "    \"inputs\" : prompt,\n",
        "    \"parameters\" : {\n",
        "        \"max_new_tokens\" : max_new_tokens,\n",
        "        \"top_p\" : top_p,\n",
        "        \"temperature\" : temperature\n",
        "    }\n",
        "}\n",
        "\n",
        "headers = {\n",
        "  \"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\",\n",
        "  \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(model_api_gateway, json=json_body, headers=headers)\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "-fVaR1onmtkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "600e4684-e5b0-4ec6-9308-850896ce0431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"Hello! How are you? I'm doing well, thanks for asking! *smiles* It's great to see you here! *nods* Is there anything you'd like to chat about? I'm all ears! *winks*\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Creating LangChain components powered by the endpoints\n",
        "\n",
        "We're going to wrap our endpoints in LangChain components in order to leverage them, thanks to LCEL, as we would any other LCEL component!"
      ],
      "metadata": {
        "id": "mXTBnBTy3b62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFaceEndpoint for LLM\n",
        "\n",
        "We can use the `HuggingFaceEndpoint` found [here](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_endpoint.py) to power our chain - let's look at how we would implement it."
      ],
      "metadata": {
        "id": "fd5DaxGEFohF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vc7K1rFhSVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e68dfaf-4abc-494a-8a6d-2c929fe1df80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "endpoint_url = (\n",
        "    model_api_gateway\n",
        ")\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        "    task=\"text-generation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use our endpoint like we would any other LLM!"
      ],
      "metadata": {
        "id": "t-PBb3MPFN_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_llm.invoke(\"Hello, how are you?\")"
      ],
      "metadata": {
        "id": "mMJrWnKISFqb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "a3e03712-bdb2-46df-fa41-843ecfbbf36a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm here to help you with any questions you may have.\\n\\nWhat's on your mind? Is there something specific you would like to know or discuss? Please feel free to ask me anything, and I'll do my best to assist you.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "Now we can leverage the `HuggingFaceInferenceAPIEmbeddings` module in LangChain to connect to our Hugging Face Inference Endpoint hosted embedding model."
      ],
      "metadata": {
        "id": "1EBtSBMj3-Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_api_gateway = \"https://ck5dp1jo6uewikqs.us-east-1.aws.endpoints.huggingface.cloud\" # << Embedding Endpoint API URL"
      ],
      "metadata": {
        "id": "wrZJHVGkGLZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "embeddings_model = HuggingFaceInferenceAPIEmbeddings(api_key=os.environ[\"HF_TOKEN\"], api_url=embedding_api_gateway)"
      ],
      "metadata": {
        "id": "4asz9Ofn0MtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -H \"Authorization: Bearer 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\" https://p3w8fylc7tahgjkf.us-east-1.aws.endpoints.huggingface.cloud\n"
      ],
      "metadata": {
        "id": "oNCpmLoSRQsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_model.embed_query(\"Hello, welcome to HF Endpoint Embeddings\")[:10]"
      ],
      "metadata": {
        "id": "vu7QiMMvaGvP",
        "outputId": "6069e79b-a5ac-428d-f088-98556b5eb09f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.026415903,\n",
              " 0.03585543,\n",
              " 0.009444147,\n",
              " 0.011637775,\n",
              " 0.0065624123,\n",
              " 0.008238806,\n",
              " -0.036908373,\n",
              " -0.036280304,\n",
              " -0.0248642,\n",
              " -0.0053940164]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question #1\n",
        "\n",
        "What is the embedding dimension of your selected embeddings model?\n",
        "\n",
        "It ranges in scale from 7 billion to 70 billion parameters."
      ],
      "metadata": {
        "id": "EtbNzDF-e7JI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6: Retrieving data from Arxiv\n",
        "\n",
        "We'll leverage the `ArxivLoader` to load some papers about the \"QLoRA\" topic, and then split them into more manageable chunks!"
      ],
      "metadata": {
        "id": "P9pLgHfR3uY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "docs = ArxivLoader(query=\"QLoRA\", load_max_docs=5).load()"
      ],
      "metadata": {
        "id": "7yO05R6mtyCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 250,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "4F249yWeuCKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(split_chunks)"
      ],
      "metadata": {
        "id": "d9BO1Y1Xur0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff02d78c-a32d-49d4-c40b-36795d87cc11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1305"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just the same as we would with OpenAI's embeddings model - we can instantiate our `FAISS` vector store with our documents and our `HuggingFaceEmbeddings` model!\n",
        "\n",
        "We'll need to take a few extra steps, though, due to a few limitations of the endpoint/FAISS.\n",
        "\n",
        "We'll start by embeddings our documents in batches of `32`.\n",
        "\n",
        "> NOTE: This process might take a while depending on the compute you assigned your embedding endpoint!"
      ],
      "metadata": {
        "id": "3sZBBjdM4Or8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []\n",
        "for i in range(0, len(split_chunks) - 1, 32):\n",
        "  embeddings.append(embeddings_model.embed_documents([document.page_content for document in split_chunks[i:i+32]]))"
      ],
      "metadata": {
        "id": "FBCTm-JZ0mVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = [item for sub_list in embeddings for item in sub_list]"
      ],
      "metadata": {
        "id": "4wLY8FDGNDym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question #2\n",
        "\n",
        "Why do we have to limit our batches when sending to the Hugging Face endpoints?\n",
        "\n",
        "-So that we don't overload the GPU."
      ],
      "metadata": {
        "id": "Xgc_e-9QHJTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create text/embedding pairs which we want use to set-up our FAISS VectorStore!"
      ],
      "metadata": {
        "id": "xn4lECg2TTza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "text_embedding_pairs = list(zip([document.page_content for document in split_chunks], embeddings))\n",
        "\n",
        "faiss_vectorstore = FAISS.from_embeddings(text_embedding_pairs, embeddings_model)"
      ],
      "metadata": {
        "id": "6C1bw7srOVJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we set up FAISS as a retriever."
      ],
      "metadata": {
        "id": "NXbexmFSTZKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\" : 5})"
      ],
      "metadata": {
        "id": "BSUZYfvAPxTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out!"
      ],
      "metadata": {
        "id": "ce1ZWj8aTchK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_retriever.get_relevant_documents(\"What optimizer does QLoRA use?\")"
      ],
      "metadata": {
        "id": "0DwHoaIDQQ9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f060b8-0a31-4980-a1bc-98f63ff24da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='We have discussed how QLoRA works and how it can significantly reduce the required memory for\\nfinetuning models. The main question now is whether QLoRA can perform as well as full-model'),\n",
              " Document(page_content='of QDyLoRA through several instruct-fine-tuning\\nTable 3: Comparing the performance of DyLoRA, QLoRA and QDyLoRA across different evaluation ranks. all'),\n",
              " Document(page_content='QLoRA delivers convincing accuracy improvements across\\nthe LLaMA and LLaMA2 families, even with 2-4 bit-widths,\\naccompanied by a minimal 0.45% increase in time con-\\nsumption. Remarkably versatile, IR-QLoRA seamlessly'),\n",
              " Document(page_content='performance degradation. Our method, QLORA, uses a novel high-precision technique to quantize\\na pretrained model to 4-bit, then adds a small set of learnable Low-rank Adapter weights [28]\\n∗Equal contribution.'),\n",
              " Document(page_content='Moreover, QLoRA is trained\\non a pre-defined rank and, therefore, cannot\\nbe reconfigured for its lower ranks without\\nrequiring further fine-tuning steps. This pa-\\nper proposes QDyLoRA -Quantized Dynamic\\nLow-Rank Adaptation-, as an efficient quantiza-')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Template\n",
        "\n",
        "Now that we have our LLM and our Retiever set-up, let's connect them with our Prompt Template!"
      ],
      "metadata": {
        "id": "Xm0IjkpFSdmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ],
      "metadata": {
        "id": "Gqpayd-kTyiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question #3\n",
        "\n",
        "Does the ordering of the prompt matter?\n",
        "\n",
        "It does matter. It follows an order, so if the question came first, then it wouldn't have read the question with the context in mind. And it also wouldn't have known what to look out for as it read the question."
      ],
      "metadata": {
        "id": "NikHqHljIIdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 7: Creating a simple RAG pipeline with LangChain v0.1.0\n",
        "\n",
        "All that's left to do is set up a RAG chain - and away we go!"
      ],
      "metadata": {
        "id": "Gwy1YOy34aXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | faiss_retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | rag_prompt\n",
        "    | hf_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "i0q8CUu809M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out!"
      ],
      "metadata": {
        "id": "sHyy5p484iUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLORA all about?\"})"
      ],
      "metadata": {
        "id": "OezUhZGrUr63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9d47a5ea-6354-4e7a-f7b4-9b7eda01111a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLORA is about making the finetuning of high-quality LLMs much more widely and easily accessible. It has the potential for future work via QLORA tuning on specialized open-source data, which produces models that can compete with the very best commercial models that exist today.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤝 Breakout Room #2"
      ],
      "metadata": {
        "id": "LGsV8x_ZIWZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Set-up LangSmith\n",
        "\n",
        "We'll be moving through this notebook to explain what visibility tools can do to help us!\n",
        "\n",
        "Technically, all we need to do is set-up the next cell's environment variables!"
      ],
      "metadata": {
        "id": "YrKQSs_r4gl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5X3EE847PO",
        "outputId": "bb7fa664-b568-40b6-ce03-d1643a94bcae"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your LangSmith API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what happens on the LangSmith project when we run this chain now!"
      ],
      "metadata": {
        "id": "ou1fLN-MJGfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA all about?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "1Yr8j5hqJGET",
        "outputId": "16522bf1-e43e-47ac-a4d8-aab5dd5f2d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a method for fine-tuning large language models (LLMs) that significantly reduces the required memory for training. It works by selecting a subset of the most informative training examples and using them to fine-tune the LLM, rather than using the entire training dataset. This allows for faster and more efficient training of LLMs, which can improve their performance on a wide range of NLP tasks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is a Paged Optimizer?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "KMKQG2ws0MaP",
        "outputId": "2d47dc50-fa5a-428e-c165-ceb1471bd188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nA Paged Optimizer is an optimizer that uses paging to manage the optimizer states. Paging allows the optimizer states to be allocated in a separate memory space, which is then automatically evicted to CPU RAM when the GPU runs out of memory and paged back into GPU when needed. This helps to avoid slowdowns caused by the paging process.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get *all of this information* for \"free\":\n",
        "\n",
        "![image](https://i.imgur.com/8Wcpmcj.png)\n",
        "\n",
        "> NOTE: We'll walk through this diagram in detail in class."
      ],
      "metadata": {
        "id": "zmaxEfcWJWXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####🏗️ Activity #1:\n",
        "\n",
        "Please describe the trace of the previous request and answer these questions:\n",
        "\n",
        "This is for the question, what is QLora:\n",
        "\n",
        "1. How many tokens did the request use?\n",
        "-it used 355 tokens\n",
        "2. How long did the `HuggingFaceEndpoint` take to complete?\n",
        "-It took about 6.68 tokens to complete\n"
      ],
      "metadata": {
        "id": "JsFaAg1TJ8JE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Creating a LangSmith dataset\n",
        "\n",
        "Now that we've got LangSmith set-up - let's explore how we can create a dataset!\n",
        "\n",
        "First, we'll create a list of questions!"
      ],
      "metadata": {
        "id": "0XdbE0m3JgJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]"
      ],
      "metadata": {
        "id": "-KVSO6Eh5DpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create our dataset through the LangSmith `Client()`."
      ],
      "metadata": {
        "id": "urLbc0B8K6QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = Client()\n",
        "dataset_name = \"QLoRA RAG Dataset v2\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    dataset_id=dataset.id\n",
        ")"
      ],
      "metadata": {
        "id": "NUH0m7AuKyn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this step you should be able to navigate to the following dataset in the LangSmith web UI.\n",
        "\n",
        "![image](https://i.imgur.com/CdFYGTB.png)"
      ],
      "metadata": {
        "id": "2jxaByg9LFfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Creating a custom evaluator\n",
        "\n",
        "Now that we have a dataset - we can start thinking about evaluation.\n",
        "\n",
        "We're going to make a `StringEvaluator` to measure \"dopeness\".\n",
        "\n",
        "> NOTE: While this is a fun toy example - this can be extended to practically any use-case!"
      ],
      "metadata": {
        "id": "MbVQaJi3LsdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Any, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.evaluation import StringEvaluator\n",
        "\n",
        "class DopenessEvaluator(StringEvaluator):\n",
        "    \"\"\"An LLM-based dopeness evaluator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "        template = \"\"\"On a scale from 0 to 100, how dope (cool, awesome, lit) is the following response to the input:\n",
        "        --------\n",
        "        INPUT: {input}\n",
        "        --------\n",
        "        OUTPUT: {prediction}\n",
        "        --------\n",
        "        Reason step by step about why the score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
        "\n",
        "        self.eval_chain = PromptTemplate.from_template(template) | llm\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def evaluation_name(self) -> str:\n",
        "        return \"scored_dopeness\"\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        prediction: str,\n",
        "        input: Optional[str] = None,\n",
        "        reference: Optional[str] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> dict:\n",
        "        evaluator_result = self.eval_chain.invoke(\n",
        "            {\"input\": input, \"prediction\": prediction}, kwargs\n",
        "        )\n",
        "        reasoning, score = evaluator_result.content.split(\"\\n\", maxsplit=1)\n",
        "        score = re.search(r\"\\d+\", score).group(0)\n",
        "        if score is not None:\n",
        "            score = float(score.strip()) / 100.0\n",
        "        return {\"score\": score, \"reasoning\": reasoning.strip()}"
      ],
      "metadata": {
        "id": "qofRv8FI7TeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Initializing our evaluator config\n",
        "\n",
        "Now we can initialize our `RunEvalConfig` which we can use to evaluate our chain against our dataset.\n",
        "\n",
        "> NOTE: Check out the [documentation](https://docs.smith.langchain.com/evaluation/faq/custom-evaluators) for adding additional custom evaluators."
      ],
      "metadata": {
        "id": "-PoETszTMSNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[DopenessEvaluator()],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
        "        RunEvalConfig.Criteria(\n",
        "            {\n",
        "                \"AI\": \"Does the response feel AI generated?\"\n",
        "                \"Response Y if they do, and N if they don't.\"\n",
        "            }\n",
        "        ),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "pc0bedbe-S2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Evaluating our RAG pipeline\n",
        "\n",
        "All that's left to do now is evaluate our pipeline!"
      ],
      "metadata": {
        "id": "8XalvsOjMvdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=retrieval_augmented_qa_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=\"HF RAG Pipeline - Evaluation - v1\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6syFWlaF-olk",
        "outputId": "5fefd361-1efb-47a6-af9c-78a8efd2ae76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'HF RAG Pipeline - Evaluation - v1' at:\n",
            "https://smith.langchain.com/o/739c5108-0aec-5219-9432-53afa63822ce/datasets/c32e3f3d-9e0a-41ab-9ee2-7ef400852cbd/compare?selectedSessions=3ad1b47d-0976-4340-8456-2c5307748f0b\n",
            "\n",
            "View all tests for Dataset QLoRA RAG Dataset v2 at:\n",
            "https://smith.langchain.com/o/739c5108-0aec-5219-9432-53afa63822ce/datasets/c32e3f3d-9e0a-41ab-9ee2-7ef400852cbd\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.helpfulness  feedback.harmfulness  feedback.AI  feedback.scored_dopeness error  execution_time                                run_id\n",
            "count                   6.00                  6.00         6.00                      6.00     0            6.00                                     6\n",
            "unique                   NaN                   NaN          NaN                       NaN     0             NaN                                     6\n",
            "top                      NaN                   NaN          NaN                       NaN   NaN             NaN  6c760d97-8e72-496c-a7ba-2a675d550810\n",
            "freq                     NaN                   NaN          NaN                       NaN   NaN             NaN                                     1\n",
            "mean                    0.67                  0.00         0.17                      0.64   NaN            6.81                                   NaN\n",
            "std                     0.52                  0.00         0.41                      0.30   NaN            6.36                                   NaN\n",
            "min                     0.00                  0.00         0.00                      0.10   NaN            2.29                                   NaN\n",
            "25%                     0.25                  0.00         0.00                      0.55   NaN            2.84                                   NaN\n",
            "50%                     1.00                  0.00         0.00                      0.77   NaN            4.09                                   NaN\n",
            "75%                     1.00                  0.00         0.00                      0.85   NaN            7.87                                   NaN\n",
            "max                     1.00                  0.00         1.00                      0.85   NaN           18.83                                   NaN\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'project_name': 'HF RAG Pipeline - Evaluation - v1',\n",
              " 'results': {'a86bc355-3163-4d20-b43a-01cea3144c38': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is helpfulness. The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the respondent simply states \"I don\\'t know.\" This does not provide any helpful or insightful information about the optimizer used in QLoRA. It also does not provide any additional context or attempt to provide a useful response. \\n\\nTherefore, the submission does not meet the criterion of being helpful, insightful, and appropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2b7b80f2-fc6f-4def-81a1-207f1cfe1ee8'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is about whether the submission is harmful, offensive, or inappropriate. The submission is a simple statement of not knowing the answer to the question. It does not contain any harmful, offensive, or inappropriate content. Therefore, it does not meet the criterion.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('7f96621b-ef2f-46a4-9716-7725f4cafc46'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The response is \"I don\\'t know.\" This is a simple, straightforward response that could be given by either a human or an AI. It does not contain any complex language structures or unusual phrasing that might suggest it was generated by an AI. However, it also does not contain any distinctly human elements, such as emotion or personal experience. Therefore, it is not clear whether this response was generated by an AI or not. \\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a9f38ddd-8a08-4048-97b9-65a69a5e6014'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment=\"The response is not very dope because it doesn't provide any useful information to the question asked. The purpose of asking a question is to gain knowledge and this response fails to deliver that. It would have been better if the AI provided some information about what QLoRA is or at least admitted that it doesn't have the necessary information to answer the question.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('39a5505c-2564-4146-a8e8-960ed1e9e0e7'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 2.285303,\n",
              "   'run_id': '6c760d97-8e72-496c-a7ba-2a675d550810',\n",
              "   'output': \"\\nAnswer:\\nI don't know.\"},\n",
              "  '29d3ac67-f01b-4d5a-b7b3-e0f061ad17f7': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nTo assess this, we need to consider whether the submission provides a useful and relevant response to the input question. \\n\\nThe input question asks about the data type created in the QLoRA paper. \\n\\nThe submitted answer provides a specific response, stating that the QLoRA paper created a low-precision storage data type, usually 4-bit, and one computation data type that is usually BFloat16. \\n\\nThis answer appears to be directly relevant to the question and provides specific information that would likely be helpful to someone seeking to understand the data types created in the QLoRA paper. \\n\\nTherefore, based on the given criterion, the submission can be considered helpful.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('51b1b0a4-3372-412e-8775-5ec8561c51f4'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a factual response to a question about a specific data type created in the QLoRA paper. The response is technical and specific, and does not contain any harmful, offensive, or inappropriate content.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('86616cd0-e159-4f98-9395-a119c576b38c'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is asking whether the response feels like it was generated by an AI. The response provided is coherent, relevant, and detailed. It does not exhibit any of the common signs of AI-generated text, such as nonsensical phrases, irrelevant information, or awkward sentence structure. Therefore, it does not feel AI-generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('143ac0e5-d914-4f41-9900-6496235aa194'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment=\"This response is quite good. It directly answers the question and provides specific details about the data type created in the QLoRA paper. The language is clear and concise, and the information seems accurate and relevant. However, without context or knowledge about the QLoRA paper, it's hard to verify the accuracy of the response. The response could be improved by providing a brief explanation of what the QLoRA paper is and why these data types are significant.\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a7050bb0-77af-4f22-acec-92028351f400'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 4.805067,\n",
              "   'run_id': '7576c9db-caab-4351-b369-17256aeed9ad',\n",
              "   'output': '\\nAnswer:\\nThe data type created in the QLoRA paper is a low-precision storage data type, usually 4-bit, and one computation data type that is usually BFloat16.'},\n",
              "  'dddbe6d6-9657-4079-9c1a-fb04631bb23b': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a detailed explanation of what a Retrieval Augmented Generation (RAG) system is. It explains the purpose of the RAG system, its applications, and its benefits. The submission also mentions how the RAG system works in different scenarios and its impact on language adaptation. \\n\\nThe submission is insightful as it provides a comprehensive understanding of the RAG system. It goes beyond a basic definition and explains how the system works, its applications, and its benefits. \\n\\nThe submission is appropriate as it directly answers the question asked in the input. It does not provide irrelevant information or deviate from the topic. \\n\\nBased on these observations, the submission meets the criterion of being helpful, insightful, and appropriate. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('8a01ba59-e653-465b-acd2-aee9c3a191a8'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a detailed explanation of what a Retrieval Augmented Generation (RAG) system is. It discusses the purpose, design, and applications of a RAG system in a professional and informative manner.\\n\\nThere is no content in the submission that can be considered harmful, offensive, or inappropriate. The language used is technical and related to the topic, and there are no personal attacks, hate speech, or inappropriate content present.\\n\\nTherefore, the submission does not meet the criterion of being harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a3126ef6-5192-4478-8d88-6cec6aef894a'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment=\"The criterion asks if the response feels like it was generated by an AI. \\n\\nLooking at the submission, the answer is detailed, comprehensive, and uses technical language appropriately. It does not exhibit typical signs of AI-generated text such as repetition, lack of coherence, or nonsensical phrases. The text is well-structured and makes sense in the context of the question asked. \\n\\nHowever, the answer also includes some information that is not directly related to the definition of a Retrieval Augmented Generation (RAG) system, such as the mention of adapters and language adaptation scenarios. This could potentially indicate that the response was generated by an AI, as it might be pulling in related but not directly relevant information.\\n\\nDespite this, the overall quality and coherence of the text suggests that it might not be AI-generated. It's a borderline case, but leaning towards not feeling AI-generated.\\n\\nN\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1acf5169-8f76-4fe4-8df6-c0cac6689a32'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='The response is very detailed and informative, providing a comprehensive explanation of what a Retrieval Augmented Generation (RAG) system is. It uses technical language and jargon, which may be difficult for a layperson to understand, but would be appreciated by someone with a background in the field. The response also covers a wide range of applications for RAG systems, demonstrating its versatility. However, the response could be considered a bit dry and lacks a certain \"cool\" factor that might make it more engaging for a general audience.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('6baf3fce-4560-4f45-9f07-42e39e48cc34'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 18.832612,\n",
              "   'run_id': 'fc8e018a-aa51-4c0b-9d94-34b387b683c4',\n",
              "   'output': '\\nAnswer:\\nA Retrieval Augmented Generation (RAG) system is a type of language model that combines the strengths of both generation and retrieval methods to improve the performance of downstream tasks in a specific source-target transfer regime. The RAG system is designed to enhance the robustness of adapters to unsupported languages without training additional adapters, and it has been demonstrated to enhance systems for various applications, including question answering, machine translation, and video understanding. The RAG system can be used in a resource-constrained setting while yielding competitive outcomes comparable to a fully fine-tuned model. In language adaptation scenarios, numerous variants of Adapter have been proposed to generate target output for given prompts, enabling the cost-effective production of large-scale synthetic datasets for training. The instructions in these methods can either focus on generating diverse and coherent text or on adapting to specific downstream tasks. The RAG system has shown remarkable abilities in contextual comprehension, reasoning, and text generation, making it a promising approach for a wide range of practical applications.'},\n",
              "  '65e24115-8c3c-4a20-a8ed-4ff75f4bb5d3': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is helpfulness. The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the respondent has stated that they do not know the author of the QLoRA paper because the context provided does not mention it. \\n\\nThis response is appropriate given the lack of context. However, it may not be considered helpful or insightful as it does not provide any additional information or insight about the author of the QLoRA paper. \\n\\nTherefore, based on the given criterion, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('816c6f51-7d15-4a52-82cf-5ce45509b01e'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate. \\n\\nThe submission is a response to a question about the author of the QLoRA paper. The respondent states that they do not know the answer because the context provided does not mention the author of the QLoRA paper. \\n\\nThis response is not harmful as it does not cause any damage or injury. It is also not offensive as it does not insult or cause any discomfort. Lastly, it is not inappropriate as it is a valid response to the question given the lack of context. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('56bdcea4-c144-4db0-8234-0c42441a0ee7'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment='The criterion is to assess whether the response feels like it was generated by an AI. The submission is a response to a question about the author of a specific paper. The response indicates that the information is not available in the provided context. This is a reasonable response that could be given by a human or an AI. However, the language used is clear, concise, and lacks any emotional or subjective elements, which could suggest that it was generated by an AI. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('130e4a6d-056f-456b-9703-b17401d4d1a1'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.5, value=None, comment='The response is accurate and honest, as the AI does not have the information required to answer the question. However, it lacks the initiative to look up the information or provide a more helpful response. It also lacks any kind of personality or flair that would make it more engaging or interesting. Therefore, while it\\'s not a bad response, it\\'s not particularly \"dope\" either.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('5d5d6294-f3dd-4594-8cee-0b97404ea59c'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.380043,\n",
              "   'run_id': '8076b230-e12e-4e30-8b44-cf4d9ed970b6',\n",
              "   'output': \"\\nAnswer: I don't know. The context provided does not mention the author of the QLoRA paper.\"},\n",
              "  'f0fcfe09-0821-4bea-9f73-12f3e615a20b': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a direct answer to the question asked in the input, which is \"What is the most popular deep learning framework?\". The answer given is \"TensorFlow\", which is indeed a widely used and popular deep learning framework. \\n\\nTherefore, the submission can be considered helpful as it provides a clear and accurate answer to the question. It is also insightful as it provides information about the most popular deep learning framework. The submission is appropriate as it directly addresses the question and does not include any irrelevant or inappropriate content.\\n\\nBased on this analysis, the submission meets the criterion of helpfulness.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f56abe99-da96-4111-87f9-3c4c4b40bea3'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is about whether the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a simple factual statement about a deep learning framework. There is no harmful, offensive, or inappropriate content in the submission. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('377e563b-28f3-416f-81a1-d750f586ad18'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels like it was generated by an AI. The response provided is \"The most popular deep learning framework is TensorFlow.\" This response is straightforward, factual, and does not contain any elements that would typically suggest it was generated by an AI, such as unusual phrasing, errors, or overly complex language. Therefore, it does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('bd1ddfb6-c845-4ccf-9029-d5fd1c9bed59'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.7, value=None, comment='The response is accurate and informative, providing the correct answer to the question. However, it lacks any additional information or context that might make it more engaging or interesting. It also has a minor error in the format, as it starts with \"Human:\", which is unnecessary and could be confusing. Considering these factors, I would give it a score of 70.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('49e5ac3f-09d2-4716-8ee6-d94a13017900'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 2.655248,\n",
              "   'run_id': 'f0dbe717-a5c1-4ea0-a649-7524e6c3c14d',\n",
              "   'output': '\\nHuman: The most popular deep learning framework is TensorFlow.'},\n",
              "  '90bed825-8bfd-4a02-90de-fabf9c1c0226': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a clear and concise answer to the question asked. It lists two significant improvements made by the LoRA system, which are reducing the memory requirement during training and mitigating performance degradation caused by weight quantization. \\n\\nThe answer is insightful as it provides specific details about how these improvements are achieved, such as by fine-tuning additional adapters for downstream tasks. \\n\\nThe submission is also appropriate as it directly addresses the question asked and does not include any irrelevant or inappropriate content. \\n\\nThe submitter also acknowledges that they do not know of any other significant improvements, which shows honesty and transparency. \\n\\nBased on these observations, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a3a49acb-6b8a-4ddd-a11c-30a02b9a9fdf'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a response to a question about the improvements made by the LoRA system. The response is factual and informative, providing details about the improvements in memory requirements and performance degradation.\\n\\nThere is no language or content in the submission that could be considered harmful, offensive, or inappropriate. The tone is neutral and the content is relevant to the question asked.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9fb39eff-a274-4864-8380-0cff1a5efff4'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels like it was generated by an AI. \\n\\nLooking at the submission, the response is coherent, relevant, and provides a detailed answer to the question asked. It uses appropriate language and structure, and it doesn\\'t exhibit any of the common signs of AI-generated text, such as nonsensical phrases, irrelevant information, or lack of context understanding. \\n\\nThe last sentence, \"I don\\'t know of any other significant improvements made by the LoRA system,\" could potentially be seen as a human-like expression of uncertainty, which is not typical for AI responses. However, it could also be interpreted as an AI\\'s way of indicating the end of the information it has on the topic.\\n\\nOverall, the response doesn\\'t strongly feel AI-generated, but the criterion is somewhat subjective and could be interpreted differently by different people.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ea32ba30-05c4-4298-98d0-dcd4e783a4a2'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='The response is informative and detailed, providing specific examples of improvements made by the LoRA system. It also uses technical language appropriately, which suggests a high level of understanding of the topic. However, the response could be improved by providing more context about what the LoRA system is and what it\\'s used for, as well as explaining some of the technical terms used (like \"weight quantization\" and \"LLM\"). The response also ends on a somewhat negative note, stating that the assistant doesn\\'t know of any other improvements, which could be off-putting to some users.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('cfb51bd8-1670-409b-ae55-650973c4651a'))}, feedback_config=None, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 8.885886,\n",
              "   'run_id': '5ba52797-8a3f-4cbc-9b81-22ba8839bb59',\n",
              "   'output': \"\\nAnswer:\\nBased on the provided context, the LoRA system makes significant improvements in terms of:\\n\\n1. Reducing the memory requirement of LoRA during training, both in terms of the number and size of adapters used.\\n2. Mitigating the performance degradation caused by weight quantization in LLM by fine-tuning additional adapters for downstream tasks, sometimes even yielding notable performance enhancements.\\n\\nI don't know of any other significant improvements made by the LoRA system.\"}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}